{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 3 - Lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de texto con N-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de corpus de textos en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Función para cargar el archivo de texto\n",
    "def load_text(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Función para limpiar el texto\n",
    "def clean_text(text):\n",
    "    # Eliminar contenido entre corchetes\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Convertir texto a minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Eliminar puntuación, excepto puntos finales que delimitan oraciones\n",
    "    text = re.sub(r'[^\\w\\s\\.]', '', text)\n",
    "    \n",
    "    # Opcional: eliminar números si no son relevantes\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Eliminar espacios adicionales\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Cargar el texto en español\n",
    "filename = 'opensubtitles_es.txt'\n",
    "raw_text = load_text(filename)\n",
    "\n",
    "# Limpiar el texto\n",
    "cleaned_text = clean_text(raw_text)\n",
    "\n",
    "# Guardar el texto limpio\n",
    "with open('opensubtitles_es_limpio.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_text)\n",
    "\n",
    "# Cargar el texto en ingles\n",
    "filename = 'opensubtitles_en.txt'\n",
    "raw_text = load_text(filename)\n",
    "\n",
    "# Limpiar el texto\n",
    "cleaned_text = clean_text(raw_text)\n",
    "\n",
    "# Guardar el texto limpio\n",
    "with open('opensubtitles_en_limpio.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de N-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 1: Cargar el Archivo de Texto Limpio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el texto limpio en una variable\n",
    "with open('opensubtitles_es_limpio.txt', 'r', encoding='utf-8') as file:\n",
    "    text_es = file.read()\n",
    "\n",
    "with open('opensubtitles_en_limpio.txt', 'r', encoding='utf-8') as file:\n",
    "    text_en = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2: Tokenización del Texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens con puntuación: ['sr', '.', 'bond', 'sr', '.', 'bond', 'qué', 'bueno', 'que', 'lo']\n",
      "Tokens con puntuación: ['fall', 'by', 'scott', 'miller', 'taste', 'for', 'life', 'by', 'scott', 'miller']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Incluir signos de puntuación en la tokenización\n",
    "tokens_es = re.findall(r'\\b\\w+\\b|[.!?]', text_es.lower())\n",
    "tokens_en = re.findall(r'\\b\\w+\\b|[.!?]', text_en.lower())\n",
    "print(\"Tokens con puntuación:\", tokens_es[:10])\n",
    "print(\"Tokens con puntuación:\", tokens_en[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 3: Construcción del Modelo de N-Gramas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "n = 3  # Cambia a trigramas\n",
    "\n",
    "# Crear el modelo de trigramas\n",
    "model_es = defaultdict(Counter)\n",
    "model_en = defaultdict(Counter)\n",
    "\n",
    "# en español\n",
    "for i in range(len(tokens_es) - n + 1):\n",
    "    gram = tuple(tokens_es[i:i + n - 1])  # Dos primeras palabras como clave\n",
    "    next_word = tokens_es[i + n - 1]      # La tercera palabra es el objetivo\n",
    "    model_es[gram][next_word] += 1  # Incrementamos la cuenta para el siguiente token\n",
    "\n",
    "# en inglés\n",
    "for i in range(len(tokens_en) - n + 1):\n",
    "    gram = tuple(tokens_en[i:i + n - 1])  # Dos primeras palabras como clave\n",
    "    next_word = tokens_en[i + n - 1]      # La tercera palabra es el objetivo\n",
    "    model_en[gram][next_word] += 1  # Incrementamos la cuenta para el siguiente token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 4: Generación de Texto Basado en el Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado: la casa se incendió .\n",
      "Texto generado: how are you curious about this is the fundamental resource that gives me fiirst call over to the bishop gone your ass over here sol yeah under control fucker s head especially when he finish him off me i il be in three two .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_text_with_end(model, starting_words, max_words=50):\n",
    "    result = list(starting_words)\n",
    "    for _ in range(max_words):\n",
    "        state = tuple(result[-2:])  # Últimas dos palabras\n",
    "        if state in model and model[state]:\n",
    "            next_word = random.choices(list(model[state].keys()), weights=model[state].values())[0]\n",
    "            result.append(next_word)\n",
    "            # Finalizar si la palabra generada es un signo de puntuación\n",
    "            if next_word in ['.', '!', '?']:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(result)\n",
    "\n",
    "# Probar la generación con finalización automática (español)\n",
    "starting_words_es = (\"la\", \"casa\")\n",
    "generated_text_es = generate_text_with_end(model_es, starting_words_es, 50)\n",
    "print(\"Texto generado:\", generated_text_es)\n",
    "\n",
    "# Probar la generación con finalización automática (inglés)\n",
    "starting_words_en = (\"how\", \"are\")\n",
    "generated_text_en = generate_text_with_end(model_en, starting_words_en, 50)\n",
    "print(\"Texto generado:\", generated_text_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de los Textos Generados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diversidad léxica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado en español:\n",
      "es muy peligroso . será para ti . te dije que esperes aquí . times square esquina de la foto\n",
      ". estoy segura que no pasaremos quién es . quién quién ya sabes alimentar la sopa de pollo . sólo\n",
      "sabíamos que era marica . no . bien podemos hacerlo con las cuerdas . es el único lugar en este\n",
      "apartamento . qué tipo de promesa . pero la lleva a la concurrencia . qué astuto eres mírate . has\n",
      "conocido a una hamburguesa . . . . . y tienes problemas . . . dos millones de dólares .\n",
      "\n",
      "Diversidad léxica: 0.64 \n",
      "\n",
      "Texto generado en ingles:\n",
      "it is . truth is the resume down right embarrassed to find out where the money . me . would\n",
      "it will test positive for pregnancy miss what are we prisoners or guards please gentlemen . welcome to my friend\n",
      "simone . you need to play for . . . but the legend of a dangerous species . good morning\n",
      "joe . at that compass . god this is why i moved in a second here and make something up\n",
      "johnny . he s home where the car you see toad stool softener i m okay to be the voice\n",
      "\n",
      "Diversidad léxica: 0.74\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(text):\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words)\n",
    "\n",
    "def generate_text_fixed_length(model, starting_words, num_words=50):\n",
    "    result = list(starting_words)\n",
    "    for _ in range(num_words - len(starting_words)):  # Ajustamos para alcanzar exactamente `num_words`\n",
    "        state = tuple(result[-2:])  # Últimas dos palabras\n",
    "        if state in model and model[state]:\n",
    "            # Elegir la siguiente palabra basada en las probabilidades del modelo\n",
    "            next_word = random.choices(list(model[state].keys()), weights=model[state].values())[0]\n",
    "            result.append(next_word)\n",
    "        else:\n",
    "            # Si no hay más opciones, reiniciar con un nuevo estado aleatorio\n",
    "            state = random.choice(list(model.keys()))\n",
    "            result.extend(state)\n",
    "    return ' '.join(result[:num_words])  # Limitar la salida exactamente a `num_words`\n",
    "\n",
    "# Dividir el texto en fragmentos para impresión\n",
    "def print_in_chunks(text, chunk_size=20):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        print(' '.join(words[i:i + chunk_size]))\n",
    "\n",
    "\n",
    "\n",
    "# Generar texto de muestra y calcular la diversidad léxica (español)\n",
    "starting_words_es = (\"es\", \"muy\")\n",
    "sample_text_es = generate_text_fixed_length(model_es, starting_words_es, 100)  # Genera exactamente 100 palabras\n",
    "diversity_es = lexical_diversity(sample_text_es)\n",
    "\n",
    "print(\"Texto generado en español:\")\n",
    "print_in_chunks(sample_text_es)\n",
    "print(\"\\nDiversidad léxica:\", diversity_es, \"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Generar texto de muestra y calcular la diversidad léxica (inglés)\n",
    "starting_words_en = (\"it\", \"is\")\n",
    "sample_text_en = generate_text_fixed_length(model_en, starting_words_en, 100)  # Genera exactamente 100 palabras\n",
    "diversity_en = lexical_diversity(sample_text_en)\n",
    "\n",
    "print(\"Texto generado en ingles:\")\n",
    "print_in_chunks(sample_text_en)\n",
    "print(\"\\nDiversidad léxica:\", diversity_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Longitud promedio de las oraciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud promedio de las oraciones en español: 3.391304347826087\n",
      "Longitud promedio de las oraciones en ingles: 6.769230769230769\n"
     ]
    }
   ],
   "source": [
    "def average_sentence_length(text):\n",
    "    sentences = text.split('.')\n",
    "    return sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
    "\n",
    "# Calcular la longitud promedio de las oraciones (español)\n",
    "avg_sentence_length_es = average_sentence_length(sample_text_es)\n",
    "print(\"Longitud promedio de las oraciones en español:\", avg_sentence_length_es)\n",
    "\n",
    "# Calcular la longitud promedio de las oraciones (inglés)\n",
    "avg_sentence_length_en = average_sentence_length(sample_text_en)\n",
    "print(\"Longitud promedio de las oraciones en ingles:\", avg_sentence_length_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ratio de Repetición**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio de repetición en español: 0.09\n",
      "Ratio de repetición en inglés: 0.1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def repetition_ratio(text):\n",
    "    words = text.split()\n",
    "    word_counts = Counter(words)\n",
    "    repeated_words = sum(1 for count in word_counts.values() if count > 1)\n",
    "    return repeated_words / len(words)\n",
    "\n",
    "# Ratio de repetición para el texto en español\n",
    "repetition_es = repetition_ratio(sample_text_es)\n",
    "print(\"Ratio de repetición en español:\", repetition_es)\n",
    "\n",
    "# Ratio de repetición para el texto en inglés\n",
    "repetition_en = repetition_ratio(sample_text_en)\n",
    "print(\"Ratio de repetición en inglés:\", repetition_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cantidad de Palabras Únicas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de palabras únicas en español: 64\n",
      "Cantidad de palabras únicas en inglés: 74\n"
     ]
    }
   ],
   "source": [
    "def unique_words_count(text):\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words)\n",
    "\n",
    "# Cantidad de palabras únicas en español e inglés\n",
    "unique_words_es = unique_words_count(sample_text_es)\n",
    "print(\"Cantidad de palabras únicas en español:\", unique_words_es)\n",
    "\n",
    "unique_words_en = unique_words_count(sample_text_en)\n",
    "print(\"Cantidad de palabras únicas en inglés:\", unique_words_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de Texto Con GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers adapter-transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocesamiento y tokenización del corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/achula/miniconda3/envs/cell_detection/lib/python3.8/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from peft import PeftModel, LoraConfig\n",
    "from torch.utils.data import Dataset\n",
    "import wandb\n",
    "\n",
    "# Cargar el tokenizador y el modelo GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Configuración del adaptador Lora\n",
    "peft_config = LoraConfig(\n",
    "    r=2,                      # Parámetro reducido para menos parámetros entrenables\n",
    "    lora_alpha=8,             # Ajuste de Lora alpha\n",
    "    lora_dropout=0.1,         # Mantenemos dropout para estabilidad\n",
    "    task_type=\"CAUSAL_LM\"     # Tipo de tarea\n",
    ")\n",
    "\n",
    "# Aplicar la configuración al modelo con PEFT\n",
    "model = PeftModel(model, peft_config)\n",
    "\n",
    "# Agregar el token de padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Leer el corpus y dividirlo en trozos más pequeños para acelerar el entrenamiento inicial\n",
    "with open(\"opensubtitles_en_limpio.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Solo tomar una parte del dataset para acelerar el entrenamiento inicial\n",
    "subset_size = 100000  # Tamaño reducido de datos para entrenamiento rápido\n",
    "text = text[:subset_size]\n",
    "\n",
    "# Tokenizar en fragmentos para evitar problemas de memoria\n",
    "chunk_size = 512\n",
    "tokens = []\n",
    "\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    tokenized_chunk = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    tokens.append(tokenized_chunk[\"input_ids\"])\n",
    "\n",
    "# Concatenar los fragmentos\n",
    "input_ids = torch.cat(tokens, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamiento de GPT-2 con el corpus de texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ffc548b29840b1b1aabbc81e251e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 45.2974, 'train_samples_per_second': 4.327, 'train_steps_per_second': 1.082, 'train_loss': 10.126649194834183, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=49, training_loss=10.126649194834183, metrics={'train_runtime': 45.2974, 'train_samples_per_second': 4.327, 'train_steps_per_second': 1.082, 'total_flos': 51257630785536.0, 'train_loss': 10.126649194834183, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define el dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.tokens[idx],\n",
    "            \"attention_mask\": torch.ones_like(self.tokens[idx]),\n",
    "            \"labels\": self.tokens[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(input_ids)\n",
    "\n",
    "# Configuración de argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-peft-results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,                 # Reducido para un entrenamiento rápido inicial\n",
    "    per_device_train_batch_size=4,      # Incrementado para mejorar la eficiencia\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    learning_rate=5e-5,                 # Tasa de aprendizaje incrementada\n",
    "    report_to=\"none\"                    # Desactivar WandB temporalmente\n",
    ")\n",
    "\n",
    "# Definir el entrenador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Iniciar el entrenamiento\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado:\n",
      " The future of artificial intelligence is uncertain.  In the coming\n",
      "years, a new generation of AI will emerge from the world of machine\n",
      "learning. It will be able to learn from human knowledge and make\n",
      "decisions about its future. This new AI could also be used to help\n",
      "people solve complex problems. The.\n"
     ]
    }
   ],
   "source": [
    "# Configuración del modelo y tokenizador en modo evaluación\n",
    "model.eval()\n",
    "\n",
    "# Función para generar texto\n",
    "def generate_text(prompt, min_length=40, max_length=60):  # Rango de longitud\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,          # Longitud máxima permitida\n",
    "            min_length=min_length,          # Longitud mínima para completar la idea\n",
    "            eos_token_id=tokenizer.eos_token_id,  \n",
    "            no_repeat_ngram_size=2,         # Evita repetición de frases\n",
    "            top_k=30,                       # Controla la variedad de tokens seleccionables\n",
    "            top_p=0.8,                      # Limita la probabilidad acumulada\n",
    "            temperature=0.6,                # Reduce la creatividad\n",
    "            do_sample=True,\n",
    "            early_stopping=True             # Permite detenerse en un punto natural\n",
    "        )\n",
    "        \n",
    "    # Decodificar el texto generado\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Asegurar que termine con un punto\n",
    "    if not generated_text.endswith('.'):\n",
    "        generated_text = generated_text.rstrip() + '.'\n",
    "        \n",
    "    return generated_text\n",
    "\n",
    "# Texto de inicio\n",
    "prompt_text = \"The future of artificial intelligence is\"\n",
    "\n",
    "import textwrap\n",
    "\n",
    "# Generación de texto\n",
    "generated_text = generate_text(prompt_text)\n",
    "print(\"Texto generado:\\n\", textwrap.fill(generated_text, width=70))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
