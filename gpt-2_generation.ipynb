{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de Texto Con GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers adapter-transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocesamiento y tokenización del corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import textwrap\n",
    "\n",
    "# Cargar el tokenizador y el modelo GPT-2 en español\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"datificate/gpt2-small-spanish\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"datificate/gpt2-small-spanish\")\n",
    "\n",
    "# Agregar el token de padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Leer el corpus completo\n",
    "with open(\"corpus_argentina_limpio.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenizar en fragmentos de longitud fija\n",
    "chunk_size = 512\n",
    "tokens = []\n",
    "\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    tokenized_chunk = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    tokens.append(tokenized_chunk[\"input_ids\"])\n",
    "\n",
    "# Concatenar los fragmentos\n",
    "input_ids = torch.cat(tokens, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamiento de GPT-2 con el corpus de texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7f2442e3de4f16b4d51732a8dc9296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 162.9608, 'train_samples_per_second': 2.945, 'train_steps_per_second': 1.473, 'train_loss': 3.7096323649088543, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=3.7096323649088543, metrics={'train_runtime': 162.9608, 'train_samples_per_second': 2.945, 'train_steps_per_second': 1.473, 'total_flos': 125420175360000.0, 'train_loss': 3.7096323649088543, 'epoch': 10.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.tokens[idx],\n",
    "            \"attention_mask\": torch.ones_like(self.tokens[idx]),\n",
    "            \"labels\": self.tokens[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(tokens)\n",
    "\n",
    "# Configuración de argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-spanish-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    learning_rate=1e-5,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Entrenador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Iniciar el entrenamiento completo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/achula/miniconda3/envs/cell_detection/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado:\n",
      " la selección argentina ha sido una de las mejores del torneo. El\n",
      "partido fue muy bueno y el equipo ganó el partido. Pero el último\n",
      "partido es importante, ya que es el momento más importante para la\n",
      "selección, porque es la última oportunidad de hacer que el mundo salga\n",
      "de la pobreza. Y el final.\n"
     ]
    }
   ],
   "source": [
    "# Configuración del modelo y tokenizador en modo evaluación\n",
    "model.eval()\n",
    "\n",
    "# Función para generar texto\n",
    "def generate_text(prompt, min_length=40, max_length=60):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            eos_token_id=tokenizer.eos_token_id,  \n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=30,\n",
    "            top_p=0.85,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "    # Decodificar el texto generado\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Asegurar que termine con un punto\n",
    "    if not generated_text.endswith('.'):\n",
    "        generated_text = generated_text.rstrip() + '.'\n",
    "        \n",
    "    return generated_text\n",
    "\n",
    "# Texto de inicio\n",
    "prompt_text = \"la selección argentina\"\n",
    "\n",
    "# Generación de texto\n",
    "generated_text = generate_text(prompt_text)\n",
    "print(\"Texto generado:\\n\", textwrap.fill(generated_text, width=70))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
